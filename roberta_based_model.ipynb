{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Import modules and packages which are needed for the analysis\nimport numpy as np\nimport pandas as pd\nimport os\nimport re\nimport string\nimport random\nimport warnings\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom sklearn.model_selection import StratifiedKFold\nimport tokenizers\nfrom transformers import RobertaModel, RobertaConfig\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set seed to make random input consistent for every run\ndef seed_everything(seed_num):\n    random.seed(seed_num)\n    np.random.seed(seed_num)\n    torch.manual_seed(seed_num)\n    os.environ['PYTHONHASHSEED'] = str(seed_num)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed_num)\n        torch.cuda.manual_seed_all(seed_num)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 0\nseed_everything(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/tweet-sentiment-extraction/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsub_df = pd.read_csv(path + 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1 missing row, so we need to remove it\ntrain.dropna(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **1. DATA CLEANING**"},{"metadata":{},"cell_type":"markdown","source":"* As we know,twitter tweets always have to be cleaned before we go onto modelling.So I will do some cleaning such as removing html tags and emojis etc.\n* Spelling correction will not be performed due to the run time limitation of the competition"},{"metadata":{"trusted":true},"cell_type":"code","source":"def removeURL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['text', 'selected_text']:\n    train[col] = train[col].astype('string')\n    train[col] = train[col].apply(lambda x: removeURL(x))\n    train[col] = train[col].apply(lambda x: remove_html(x))\n    train[col] = train[col].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['text'] = test['text'].astype('string')\ntest['text'] = test['text'].apply(lambda x: removeURL(x))\ntest['text'] = test['text'].apply(lambda x: remove_html(x))\ntest['text'] = test['text'].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **2. DATA LOADER**"},{"metadata":{},"cell_type":"markdown","source":"In this project, I need to select words which have correct sentiment in the text. Thus, I structured this problem similar to the question/answer problem with:\n- Question is the sentiment.\n- Answer is the text.\n- Training with selected text.\n\nThe model I am using is Roberta model. The tokenizer structure is:\n![image.png](attachment:image.png)\n\nSo I need to determine:\n- The starting index of the token of the selected text in the text token.\n- The offset of the token of the selected text. The offset is the number of token from the starting token to the end token of the selected text tokens.\n\nThen I can get the selected text which corresponds to the sentiment given","attachments":{"image.png":{"image/png":"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"}}},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a class to load data set\n\nclass GetDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_len = 99):\n        self.df = df\n        self.max_len = max_len\n        self.labeleb = 'selected_text' in df\n        self.tokenizer = tokenizers.ByteLevelBPETokenizer('../input/roberta-base/vocab.json',\n                                                         '../input/roberta-base/merges.txt',\n                                                         lowercase = True,\n                                                         add_prefix_space = True)\n        \n    def __getitem__(self, index):\n        data = {}\n        row = self.df.iloc[index]\n        \n        #get index, mask, tweet, and offsets of selected text on each row\n        ids, masks, tweet, offsets = self.get_input_data(row)        \n        data['ids'] = ids\n        data['masks'] = masks\n        data['tweet'] = tweet\n        data['offsets'] = offsets\n        \n        #get start index, end index of selected text on each row\n        if self.labeled:\n            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n            data['start_idx'] = start_idx\n            data['end_idx'] = end_idx\n            \n        return data\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def get_input_data(self, row):\n        \n        #Because Roberta require a space prefix so a space is added as a prefix for each text\n        #to remove all redundent space if have, the text is splitted then join again\n        tweet = ' ' + ' '.join(row.text.lower().split())\n        \n        #index array for each row with [0] is start sentence token id and [2] is end sentence token id\n        encoding = self.tokenizer.encode(tweet)\n        sentiment_id = self.tokenizer.encode(row.sentiment).ids        \n        ids = [0] + sentiment_id + [2,2] + encoding.ids + [2]\n        \n        #get offset, 4 is the combination of start token, sentiment token and 2 stop tokens\n        offsets = [(0,0)]*4 + encoding.offsets + [(0,0)]\n        \n        #padding 1 on the remaining of the len\n        pad_len = self.max_len - len(ids)\n        if pad_len > 0:\n            ids += [1] * pad_len\n            offsets += [(0,0)] * pad_len\n            \n        #convert ids and offset to torch tensor\n        ids = torch.tensor(ids)\n        offsets = torch.tensor(offsets)\n        \n        #mask all where index = 1\n        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n        \n        return ids, masks, tweet, offsets\n    \n    def get_target_idx(self, row, tweet, offsets):\n        \n        selected_text = ' ' + ' '.join(row.selected_text.lower().split())\n        len_st = len(selected_text) - 1\n        idx0 = None\n        idx1 = None\n        \n        #get the start index and end index of character of selected text in the tweet\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]): #selected_text[0] is a space I added\n            if ' ' + tweet[ind:ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n        \n        #set 1 for characters which are the selected text in character targets array\n        char_targets = [0] * len(tweet)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1+1):\n                char_targets[ct] = 1\n                \n        \n        #so the target is getting the index of words of the selected text in the tweet\n        #the character of selected text is set to 1 in char_targets\n        #so if the word in the tweet is a word of selected text, the sum of char_targets\n        #at the positions of the word is greater than 0.\n        target_idx = []\n        for i, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1:offset2]) >0 :\n                taget_idx.append(i)\n        start_idx = target_idx[0]\n        end_idx = target_idx[-1]\n        \n        return start_idx, end_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to get train loader and validation loader\n\ndef get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n    \n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n    \n    train_loader = torch.utils.data.DataLoader(GetDataset(train_df),\n                                              batch_size=batch_size,\n                                              shuffle=True,\n                                              num_workers=2,\n                                              drop_last=True)\n    val_loader = torch.utils.data.DataLoader(GetDataset(val_df),\n                                            batch_size=batch_size,\n                                            shuffle=False,\n                                            num_workers=2)\n    dataloaders_dict = {'train': train_loader, 'val': val_loader}\n    \n    return dataloaders_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fuction to get test loader\n\ndef get_test_loader(df, batch_size=8):\n    \n    loader = torch.utils.data.DataLoader(GetDataset(df),\n                                        batch_size=batch_size,\n                                        shuffle=False,\n                                        num_workers=2)\n    return loader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **3. MODEL**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a class of roberta base model\n\nclass TweetModel(nn.Module):\n    \n    def __init__(self):\n        \n        super(TweetModel, self).__init__()\n        \n        config = RobertaConfig.from_pretrained('../input/roberta-base/config.json',\n                                              output_hidden_states=True)\n        self.roberta = RobertaModel.from_pretrained('../input/roberta-base/pytorch_model.bin',\n                                                   config=config)\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(config.hidden_size, 2)\n        nn.init.normal_(self.fc.weight, std=0.02)\n        nn.init.normal_(self.fc.bias, 0)\n        \n    def forward(self, input_ids, attention_mask):\n        \n        #get output of roberta base model\n        _, _, hs = self.roberta(input_ids, attention_mask)\n        \n        x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n        x = torch.mean(x, 0)\n        x = self.dropout(x)\n        x = self.fc(x)\n        \n        start_logits, end_logits = x.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        \n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **4. LOSS FUNCTION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    \n    ce_loss = nn.CrossEntropyLoss()\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)\n    total_loss = start_loss + end_loss\n    \n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **5. EVALUATION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_selected_text(text, start_idx, end_idx, offsets):\n    \n    selected_text = ''\n    \n    for ix in range(start_idx, end_idx+1):\n        selected_text += text[offsets[ix][0]:offsets[ix][1]]\n        #add space to between the words of the selected text\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            selected_text += ' '\n    \n    return selected_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#functions to compute jaccard score of similarity between 2 strings\ndef jaccard(str1, str2):\n    \n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    \n    return float(len(c)/(len(a)+len(b)-len(c)))\n\ndef compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n    \n    start_pred = np.argmax(start_logits)\n    end_pred = np.argmax(end_logits)\n    if start_pred > end_pred:\n        pred = text\n    else:\n        pred = get_selected_text(text, start_pred, end_pred, offsets)\n        \n    true = get_selected_text(text, start_idx, end_idx, offsets)\n    \n    return jaccard(pred, true)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **6. TRAINING**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n    \n    model.cuda()\n    \n    for epoch in range(num_epochs):\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            epoch_loss = 0.0\n            epoch_jaccard = 0.0\n            \n            for data in (dataloaders_dict[phase]):\n                ids = data['ids'].cuda()\n                masks = data['masks'].cuda()\n                tweet = data['tweet'].cuda()\n                offsets = data['offsets'].cuda()\n                start_idx = data['start_idx'].cuda()\n                end_idx = data['end_idx'].cuda()\n                \n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase=='train'):\n                    start_logits, end_logits = model(ids, masks)\n                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        \n                    epoch_loss += loss.item() * len(ids)\n                    start_idx = start_idx.cpu().detach().numpy()\n                    end_idx = end_idx.cpu().detach().numpy()\n                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    \n                    for i in range(len(ids)):\n                        jaccard_score = compute_jaccard_score(tweet[i],\n                                                             start_idx[i],\n                                                             end_idx[i],\n                                                             start_logits[i],\n                                                             end_logits[i],\n                                                             offsets[i])\n                        epoch_jaccard += jaccard_score\n                        \n            epoch_loss = epoch_loss/len(dataloaders_dict[phase].dataset)\n            epoch_jaccard = epoch_jaccard/len(dataloaders_dict[phase].dataset)\n            \n            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n            \n    torch.save(model.state_dict(), filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 5\nbatch_size = 8\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state = seed)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train, train.sentiment), start=1):\n    print(f'Fold: {fold}')\n    \n    model = TweetModel()\n    optimizer = optim.AdamW(model.parameters(), lr=1e-6, betas=(0.9, 0.999))\n    criterion = loss_fn\n    dataloaders_dict = get_train_val_loaders(train, train_idx, val_idx, batch_size)\n    \n    train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, f'roberta_fold{fold}.pth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **7. INFERENCE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = get_test_loader(test)\npredictions = []\nmodels = []\n\nfor fold in range(skf.n_splits):\n    model = TweetModel()\n    model.cuda()\n    model.load_state_dict(torch.load(f'roberta_fold{fold+1}.pth'))\n    model.eval()\n    models.append(model)\n    \nfor data in test_loader:\n    ids = data['ids'].cuda()\n    masks = data['masks'].cuda()\n    tweet = data['tweet'].cuda()\n    offsets = data['offsets'].cuda()\n    \n    start_logits = []\n    end_logits = []\n    \n    for model in models:\n        with torch.no_grad():\n            output = model(ids, masks)\n            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n            \n    start_logits = np.mean(start_logits, axis=0)\n    end_logits = np.mean(end_logits, axis=0)\n    \n    for i in range(len(ids)):    \n        start_pred = np.argmax(start_logits[i])\n        end_pred = np.argmax(end_logits[i])\n        if start_pred > end_pred:\n            pred = tweet[i]\n        else:\n            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n        predictions.append(pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MAKE SUBMISSION FILE"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df['selected_text'] = predictions\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\nsub_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}